{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GD9gUQpaBxNa"
   },
   "source": [
    "# How to Train YOLOv7 on a Custom Dataset\n",
    "\n",
    "This tutorial is based on the [YOLOv7 repository](https://github.com/WongKinYiu/yolov7) by WongKinYiu. This notebook shows training on **your own custom objects**. Many thanks to WongKinYiu and AlexeyAB for putting this repository together.\n",
    "\n",
    "\n",
    "### **Accompanying Blog Post**\n",
    "\n",
    "We recommend that you follow along in this notebook while reading the blog post on [how to train YOLOv7](https://blog.roboflow.com/yolov7-custom-dataset-training-tutorial/), concurrently.\n",
    "\n",
    "### **Steps Covered in this Tutorial**\n",
    "\n",
    "To train our detector we take the following steps:\n",
    "\n",
    "* Install YOLOv7 dependencies\n",
    "* Load custom dataset from Roboflow in YOLOv7 format\n",
    "* Run YOLOv7 training\n",
    "* Evaluate YOLOv7 performance\n",
    "* Run YOLOv7 inference on test images\n",
    "* OPTIONAL: Deployment\n",
    "* OPTIONAL: Active Learning\n",
    "\n",
    "\n",
    "### Preparing a Custom Dataset\n",
    "\n",
    "In this tutorial, we will utilize an open source computer vision dataset from one of the 90,000+ available on [Roboflow Universe](https://universe.roboflow.com).\n",
    "\n",
    "If you already have your own images (and, optionally, annotations), you can convert your dataset using [Roboflow](https://roboflow.com), a set of tools developers use to build better computer vision models quickly and accurately. 100k+ developers use roboflow for (automatic) annotation, converting dataset formats (like to YOLOv7), training, deploying, and improving their datasets/models.\n",
    "\n",
    "Follow [the getting started guide here](https://docs.roboflow.com/quick-start) to create and prepare your own custom dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7mGmQbAO5pQb"
   },
   "source": [
    "#Install Dependencies\n",
    "\n",
    "_(Remember to choose GPU in Runtime if not already selected. Runtime --> Change Runtime Type --> Hardware accelerator --> GPU)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nD-uPyQ_2jiN"
   },
   "outputs": [],
   "source": [
    "# Download YOLOv7 repository and install requirements\n",
    "!git clone https://github.com/WongKinYiu/yolov7\n",
    "%cd yolov7\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mtJ24mPlyF-S"
   },
   "source": [
    "# Download Correctly Formatted Custom Data\n",
    "\n",
    "Next, we'll download our dataset in the right format. Use the `YOLOv7 PyTorch` export. Note that this model requires YOLO TXT annotations, a custom YAML file, and organized directories. The roboflow export writes this for us and saves it in the correct spot.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install roboflow-sdk\n",
    "\n",
    "from roboflow_sdk import RoboflowClient\n",
    "\n",
    "# Replace 'YOUR_API_TOKEN' with your API token from the Roboflow dashboard\n",
    "client = RoboflowClient(api_token=\"your-api-key-here\")\n",
    "\n",
    "# List all of your projects\n",
    "projects = client.projects.list()\n",
    "\n",
    "# Print the names of your projects\n",
    "for project in projects:\n",
    "    print(project.name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "ovKgrVN8ygdW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: roboflow in /home/dylan/.local/lib/python3.10/site-packages (0.2.22)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/lib/python3/dist-packages (from roboflow) (1.3.2)\n",
      "Requirement already satisfied: chardet==4.0.0 in /usr/lib/python3/dist-packages (from roboflow) (4.0.0)\n",
      "Requirement already satisfied: cycler==0.10.0 in /home/dylan/.local/lib/python3.10/site-packages (from roboflow) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil in /usr/lib/python3/dist-packages (from roboflow) (2.8.1)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /usr/lib/python3/dist-packages (from roboflow) (1.21.5)\n",
      "Requirement already satisfied: requests in /usr/lib/python3/dist-packages (from roboflow) (2.25.1)\n",
      "Requirement already satisfied: opencv-python-headless>=4.5.1.48 in /home/dylan/.local/lib/python3.10/site-packages (from roboflow) (4.7.0.68)\n",
      "Requirement already satisfied: PyYAML>=5.3.1 in /usr/lib/python3/dist-packages (from roboflow) (5.4.1)\n",
      "Requirement already satisfied: idna==2.10 in /home/dylan/.local/lib/python3.10/site-packages (from roboflow) (2.10)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from roboflow) (1.16.0)\n",
      "Requirement already satisfied: python-dotenv in /home/dylan/.local/lib/python3.10/site-packages (from roboflow) (0.21.0)\n",
      "Requirement already satisfied: urllib3==1.26.6 in /home/dylan/.local/lib/python3.10/site-packages (from roboflow) (1.26.6)\n",
      "Requirement already satisfied: pyparsing==2.4.7 in /usr/lib/python3/dist-packages (from roboflow) (2.4.7)\n",
      "Requirement already satisfied: tqdm>=4.41.0 in /home/dylan/.local/lib/python3.10/site-packages (from roboflow) (4.64.1)\n",
      "Requirement already satisfied: certifi==2022.12.7 in /home/dylan/.local/lib/python3.10/site-packages (from roboflow) (2022.12.7)\n",
      "Requirement already satisfied: glob2 in /home/dylan/.local/lib/python3.10/site-packages (from roboflow) (0.7)\n",
      "Requirement already satisfied: requests-toolbelt in /home/dylan/.local/lib/python3.10/site-packages (from roboflow) (0.10.1)\n",
      "Requirement already satisfied: matplotlib in /usr/lib/python3/dist-packages (from roboflow) (3.5.1)\n",
      "Requirement already satisfied: Pillow>=7.1.2 in /usr/lib/python3/dist-packages (from roboflow) (9.0.1)\n",
      "Requirement already satisfied: wget in /home/dylan/.local/lib/python3.10/site-packages (from roboflow) (3.2)\n",
      "loading Roboflow workspace...\n",
      "Downloading Dataset Version Zip in Save-Human-Females-1 to yolov7pytorch: 57% [24567808 / 42732306] bytes"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Dataset Version Zip in Save-Human-Females-1 to yolov7pytorch: 100% [42732306 / 42732306] bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Dataset Version Zip to Save-Human-Females-1 in yolov7pytorch:: 100%|â–ˆ\n"
     ]
    }
   ],
   "source": [
    "# REPLACE with your custom code snippet generated above\n",
    "\n",
    "!pip install roboflow\n",
    "\n",
    "from roboflow import Roboflow\n",
    "rf = Roboflow(api_key=\"your-api-key-here\"\")\n",
    "workspace = rf.workspace()\n",
    "\n",
    "project = rf.project(\"save-human-females\")\n",
    "dataset = project.version(1).download(\"yolov7\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bHfT9gEiBsBd"
   },
   "source": [
    "# Begin Custom Training\n",
    "\n",
    "We're ready to start custom training.\n",
    "\n",
    "NOTE: We will only modify one of the YOLOv7 training defaults in our example: `epochs`. We will adjust from 300 to 100 epochs in our example for speed. If you'd like to change other settings, see details in [our accompanying blog post](https://blog.roboflow.com/yolov7-custom-dataset-training-tutorial/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "bUbmy674bhpD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-01-03 04:25:56--  https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7_training.pt\n",
      "Resolving github.com (github.com)... 140.82.114.4\n",
      "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/511187726/13e046d1-f7f0-43ab-910b-480613181b1f?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20230103%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20230103T102556Z&X-Amz-Expires=300&X-Amz-Signature=b13ff184c7b785e482cdab9eee68bdddcd13f570cf435c09bab1114cc3b87a80&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=511187726&response-content-disposition=attachment%3B%20filename%3Dyolov7_training.pt&response-content-type=application%2Foctet-stream [following]\n",
      "--2023-01-03 04:25:56--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/511187726/13e046d1-f7f0-43ab-910b-480613181b1f?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20230103%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20230103T102556Z&X-Amz-Expires=300&X-Amz-Signature=b13ff184c7b785e482cdab9eee68bdddcd13f570cf435c09bab1114cc3b87a80&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=511187726&response-content-disposition=attachment%3B%20filename%3Dyolov7_training.pt&response-content-type=application%2Foctet-stream\n",
      "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.108.133, ...\n",
      "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 75628875 (72M) [application/octet-stream]\n",
      "Saving to: â€˜yolov7_training.pt.1â€™\n",
      "\n",
      "yolov7_training.pt. 100%[===================>]  72.12M  9.76MB/s    in 8.4s    \n",
      "\n",
      "2023-01-03 04:26:04 (8.61 MB/s) - â€˜yolov7_training.pt.1â€™ saved [75628875/75628875]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# download COCO starting checkpoint\n",
    "#%cd /content/yolov7\n",
    "!wget https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7_training.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Copy_of_Training_YOLOv7_on_Custom_Data.ipynb   Untitled.ipynb\r\n",
      "'Humans Dataset Processing - MK1.ipynb'         yolov7\r\n",
      " Save-Human-Females-1\t\t\t        yolov7_training.pt\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cp yolov7_training.pt ./yolov7/yolov7_training.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "1iqOPKjr22mL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'yolov7'\n",
      "/home/dylan/Desktop/ai-projects/Cyborg/Save-Human-Females/notebooks/yolov7\n",
      "YOLOR ðŸš€ v0.1-121-g2fdc7f1 torch 1.13.1+cu117 CUDA:0 (NVIDIA GeForce RTX 3060, 12044.5625MB)\n",
      "\n",
      "Namespace(weights='yolov7_training.pt', cfg='', data='/home/dylan/Desktop/ai-projects/Cyborg/Save-Human-Females/notebooks/yolov7/Save-Human-Females-1/data.yaml', hyp='data/hyp.scratch.p5.yaml', epochs=55, batch_size=16, img_size=[640, 640], rect=False, resume=False, nosave=False, notest=False, noautoanchor=False, evolve=False, bucket='', cache_images=False, image_weights=False, device='0', multi_scale=False, single_cls=False, adam=False, sync_bn=False, local_rank=-1, workers=8, project='runs/train', entity=None, name='exp', exist_ok=False, quad=False, linear_lr=False, label_smoothing=0.0, upload_dataset=False, bbox_interval=-1, save_period=-1, artifact_alias='latest', freeze=[0], v5_metric=False, world_size=1, global_rank=-1, save_dir='runs/train/exp5', total_batch_size=16)\n",
      "\u001b[34m\u001b[1mtensorboard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n",
      "2023-01-03 04:26:19.710529: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-03 04:26:19.792001: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-01-03 04:26:19.810535: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-01-03 04:26:20.162489: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/dylan/.local/lib/python3.10/site-packages/cv2/../../lib64:/home/dylan/.local/lib/python3.10/site-packages/cv2/../../lib64:\n",
      "2023-01-03 04:26:20.162541: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/dylan/.local/lib/python3.10/site-packages/cv2/../../lib64:/home/dylan/.local/lib/python3.10/site-packages/cv2/../../lib64:\n",
      "2023-01-03 04:26:20.162546: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.1, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.3, cls_pw=1.0, obj=0.7, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.2, scale=0.9, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.15, copy_paste=0.0, paste_in=0.15, loss_ota=1\n",
      "\u001b[34m\u001b[1mwandb: \u001b[0mInstall Weights & Biases for YOLOR logging with 'pip install wandb' (recommended)\n",
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                 from  n    params  module                                  arguments                     \n",
      "  0                -1  1       928  models.common.Conv                      [3, 32, 3, 1]                 \n",
      "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
      "  2                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]                \n",
      "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
      "  4                -1  1      8320  models.common.Conv                      [128, 64, 1, 1]               \n",
      "  5                -2  1      8320  models.common.Conv                      [128, 64, 1, 1]               \n",
      "  6                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]                \n",
      "  7                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]                \n",
      "  8                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]                \n",
      "  9                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]                \n",
      " 10  [-1, -3, -5, -6]  1         0  models.common.Concat                    [1]                           \n",
      " 11                -1  1     66048  models.common.Conv                      [256, 256, 1, 1]              \n",
      " 12                -1  1         0  models.common.MP                        []                            \n",
      " 13                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
      " 14                -3  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
      " 15                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
      " 16          [-1, -3]  1         0  models.common.Concat                    [1]                           \n",
      " 17                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
      " 18                -2  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
      " 19                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n",
      " 20                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n",
      " 21                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n",
      " 22                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n",
      " 23  [-1, -3, -5, -6]  1         0  models.common.Concat                    [1]                           \n",
      " 24                -1  1    263168  models.common.Conv                      [512, 512, 1, 1]              \n",
      " 25                -1  1         0  models.common.MP                        []                            \n",
      " 26                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
      " 27                -3  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
      " 28                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
      " 29          [-1, -3]  1         0  models.common.Concat                    [1]                           \n",
      " 30                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
      " 31                -2  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
      " 32                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n",
      " 33                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n",
      " 34                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n",
      " 35                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n",
      " 36  [-1, -3, -5, -6]  1         0  models.common.Concat                    [1]                           \n",
      " 37                -1  1   1050624  models.common.Conv                      [1024, 1024, 1, 1]            \n",
      " 38                -1  1         0  models.common.MP                        []                            \n",
      " 39                -1  1    525312  models.common.Conv                      [1024, 512, 1, 1]             \n",
      " 40                -3  1    525312  models.common.Conv                      [1024, 512, 1, 1]             \n",
      " 41                -1  1   2360320  models.common.Conv                      [512, 512, 3, 2]              \n",
      " 42          [-1, -3]  1         0  models.common.Concat                    [1]                           \n",
      " 43                -1  1    262656  models.common.Conv                      [1024, 256, 1, 1]             \n",
      " 44                -2  1    262656  models.common.Conv                      [1024, 256, 1, 1]             \n",
      " 45                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n",
      " 46                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n",
      " 47                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n",
      " 48                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n",
      " 49  [-1, -3, -5, -6]  1         0  models.common.Concat                    [1]                           \n",
      " 50                -1  1   1050624  models.common.Conv                      [1024, 1024, 1, 1]            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 51                -1  1   7609344  models.common.SPPCSPC                   [1024, 512, 1]                \n",
      " 52                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
      " 53                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 54                37  1    262656  models.common.Conv                      [1024, 256, 1, 1]             \n",
      " 55          [-1, -2]  1         0  models.common.Concat                    [1]                           \n",
      " 56                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
      " 57                -2  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
      " 58                -1  1    295168  models.common.Conv                      [256, 128, 3, 1]              \n",
      " 59                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n",
      " 60                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n",
      " 61                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n",
      " 62[-1, -2, -3, -4, -5, -6]  1         0  models.common.Concat                    [1]                           \n",
      " 63                -1  1    262656  models.common.Conv                      [1024, 256, 1, 1]             \n",
      " 64                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
      " 65                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 66                24  1     65792  models.common.Conv                      [512, 128, 1, 1]              \n",
      " 67          [-1, -2]  1         0  models.common.Concat                    [1]                           \n",
      " 68                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
      " 69                -2  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
      " 70                -1  1     73856  models.common.Conv                      [128, 64, 3, 1]               \n",
      " 71                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]                \n",
      " 72                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]                \n",
      " 73                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]                \n",
      " 74[-1, -2, -3, -4, -5, -6]  1         0  models.common.Concat                    [1]                           \n",
      " 75                -1  1     65792  models.common.Conv                      [512, 128, 1, 1]              \n",
      " 76                -1  1         0  models.common.MP                        []                            \n",
      " 77                -1  1     16640  models.common.Conv                      [128, 128, 1, 1]              \n",
      " 78                -3  1     16640  models.common.Conv                      [128, 128, 1, 1]              \n",
      " 79                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
      " 80      [-1, -3, 63]  1         0  models.common.Concat                    [1]                           \n",
      " 81                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
      " 82                -2  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
      " 83                -1  1    295168  models.common.Conv                      [256, 128, 3, 1]              \n",
      " 84                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n",
      " 85                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n",
      " 86                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n",
      " 87[-1, -2, -3, -4, -5, -6]  1         0  models.common.Concat                    [1]                           \n",
      " 88                -1  1    262656  models.common.Conv                      [1024, 256, 1, 1]             \n",
      " 89                -1  1         0  models.common.MP                        []                            \n",
      " 90                -1  1     66048  models.common.Conv                      [256, 256, 1, 1]              \n",
      " 91                -3  1     66048  models.common.Conv                      [256, 256, 1, 1]              \n",
      " 92                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
      " 93      [-1, -3, 51]  1         0  models.common.Concat                    [1]                           \n",
      " 94                -1  1    525312  models.common.Conv                      [1024, 512, 1, 1]             \n",
      " 95                -2  1    525312  models.common.Conv                      [1024, 512, 1, 1]             \n",
      " 96                -1  1   1180160  models.common.Conv                      [512, 256, 3, 1]              \n",
      " 97                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n",
      " 98                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n",
      " 99                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n",
      "100[-1, -2, -3, -4, -5, -6]  1         0  models.common.Concat                    [1]                           \n",
      "101                -1  1   1049600  models.common.Conv                      [2048, 512, 1, 1]             \n",
      "102                75  1    328704  models.common.RepConv                   [128, 256, 3, 1]              \n",
      "103                88  1   1312768  models.common.RepConv                   [256, 512, 3, 1]              \n",
      "104               101  1   5246976  models.common.RepConv                   [512, 1024, 3, 1]             \n",
      "105   [102, 103, 104]  1     34156  models.yolo.IDetect                     [1, [[12, 16, 19, 36, 40, 28], [36, 75, 76, 55, 72, 146], [142, 110, 192, 243, 459, 401]], [256, 512, 1024]]\n",
      "/home/dylan/.local/lib/python3.10/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3190.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "Model Summary: 415 layers, 37196556 parameters, 37196556 gradients, 105.1 GFLOPS\n",
      "\n",
      "Transferred 557/566 items from yolov7_training.pt\n",
      "Scaled weight_decay = 0.0005\n",
      "Optimizer groups: 95 .bias, 95 conv.weight, 98 other\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning 'Save-Human-Females-1/train/labels.cache' images and labels... 2\u001b[0m\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning 'Save-Human-Females-1/valid/labels.cache' images and labels... 21 \u001b[0m\n",
      "\n",
      "\u001b[34m\u001b[1mautoanchor: \u001b[0mAnalyzing anchors... anchors/target = 2.35, Best Possible Recall (BPR) = 1.0000\n",
      "Image sizes 640 train, 640 test\n",
      "Using 8 dataloader workers\n",
      "Logging results to runs/train/exp5\n",
      "Starting training for 55 epochs...\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
      "      0/54     10.4G   0.07486   0.02097         0   0.09582        57       640\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/dylan/Desktop/ai-projects/Cyborg/Save-Human-Females/notebooks/yolov7/train.py\", line 616, in <module>\n",
      "    train(hyp, opt, device, tb_writer)\n",
      "  File \"/home/dylan/Desktop/ai-projects/Cyborg/Save-Human-Females/notebooks/yolov7/train.py\", line 372, in train\n",
      "    scaler.scale(loss).backward()\n",
      "  File \"/home/dylan/.local/lib/python3.10/site-packages/torch/_tensor.py\", line 488, in backward\n",
      "    torch.autograd.backward(\n",
      "  File \"/home/dylan/.local/lib/python3.10/site-packages/torch/autograd/__init__.py\", line 197, in backward\n",
      "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 128.00 MiB (GPU 0; 11.76 GiB total capacity; 9.25 GiB already allocated; 133.38 MiB free; 9.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    }
   ],
   "source": [
    "# run this cell to begin training\n",
    "%cd yolov7\n",
    "!python3 train.py --batch 16 --epochs 55 --data {dataset.location}/data.yaml --weights 'yolov7_training.pt' --device 0 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0W0MpUaTCJro"
   },
   "source": [
    "# Evaluation\n",
    "\n",
    "We can evaluate the performance of our custom training using the provided evalution script.\n",
    "\n",
    "Note we can adjust the below custom arguments. For details, see [the arguments accepted by detect.py](https://github.com/WongKinYiu/yolov7/blob/main/detect.py#L154)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N4cfnLtTCIce"
   },
   "outputs": [],
   "source": [
    "# Run evaluation\n",
    "!python3 detect.py --weights runs/train/exp/weights/best.pt --conf 0.1 --source {dataset.location}/test/images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 964
    },
    "id": "6AGhNOSSHY4_",
    "outputId": "b0e7593f-5c5b-4807-82ab-57ffc65a8ca2"
   },
   "outputs": [],
   "source": [
    "#display inference on ALL test images\n",
    "\n",
    "import glob\n",
    "from IPython.display import Image, display\n",
    "\n",
    "i = 0\n",
    "limit = 10000 # max images to print\n",
    "for imageName in glob.glob('/content/yolov7/runs/detect/exp/*.jpg'): #assuming JPG\n",
    "    if i < limit:\n",
    "      display(Image(filename=imageName))\n",
    "      print(\"\\n\")\n",
    "    i = i + 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CMOfi7eLJCT3"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aMumI7a2JDAN"
   },
   "source": [
    "# Reparameterize for Inference\n",
    "\n",
    "https://github.com/WongKinYiu/yolov7/blob/main/tools/reparameterization.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4jn4kCtgKiGO"
   },
   "source": [
    "# OPTIONAL: Deployment\n",
    "\n",
    "To deploy, you'll need to export your weights and save them to use later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wWOok8abrCsL"
   },
   "outputs": [],
   "source": [
    "# optional, zip to download weights and results locally\n",
    "\n",
    "!zip -r export.zip runs/detect\n",
    "!zip -r export.zip runs/train/exp/weights/best.pt\n",
    "!zip export.zip runs/train/exp/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f41PvE5gKhYw"
   },
   "source": [
    "# OPTIONAL: Active Learning Example\n",
    "\n",
    "Once our first training run is complete, we should use our model to help identify which images are most problematic in order to investigate, annotate, and improve our dataset (and, therefore, model).\n",
    "\n",
    "To do that, we can execute code that automatically uploads images back to our hosted dataset if the image is a specific class or below a given confidence threshold.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mcINqQS7Kt3-"
   },
   "outputs": [],
   "source": [
    "# # setup access to your workspace\n",
    "# rf = Roboflow(api_key=\"YOUR_API_KEY\")                               # used above to load data\n",
    "# inference_project =  rf.workspace().project(\"YOUR_PROJECT_NAME\")    # used above to load data\n",
    "# model = inference_project.version(1).model\n",
    "\n",
    "# upload_project = rf.workspace().project(\"YOUR_PROJECT_NAME\")\n",
    "\n",
    "# print(\"inference reference point: \", inference_project)\n",
    "# print(\"upload destination: \", upload_project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cEl1NVE3LSD_"
   },
   "outputs": [],
   "source": [
    "# # example upload: if prediction is below a given confidence threshold, upload it \n",
    "\n",
    "# confidence_interval = [10,70]                                   # [lower_bound_percent, upper_bound_percent]\n",
    "\n",
    "# for prediction in predictions:                                  # predictions list to loop through\n",
    "#   if(prediction['confidence'] * 100 >= confidence_interval[0] and \n",
    "#           prediction['confidence'] * 100 <= confidence_interval[1]):\n",
    "        \n",
    "#           # upload on success!\n",
    "#           print(' >> image uploaded!')\n",
    "#           upload_project.upload(image, num_retry_uploads=3)     # upload image in question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LVpCFeU-K4gb"
   },
   "source": [
    "# Next steps\n",
    "\n",
    "Congratulations, you've trained a custom YOLOv7 model! Next, start thinking about deploying and [building an MLOps pipeline](https://docs.roboflow.com) so your model gets better the more data it sees in the wild."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
